general:
  use_gpu: True # only if available
  gpu_device: "cuda" # specify CUDA device, these are 0-indexed, e.g., cuda:0, cuda:1 or others. "cuda" is the default CUDA device
  models_dir: "./models"

preprocessing:
  uni2ascii: True
  lowercase: True
  strip: True
  only_latin_letters: False
  missing_char_threshold: 0.5
  # separation in the input CSV file
  csv_sep: "\t"

# --- GRU/LSTM architecture/misc info
gru_lstm:
  # main_architecture: gru or lstm (not implemented)?
  # mode: char or word or bigram, character or word-level tokenization?
  # bidirectional: f True, becomes a bidirectional GRU/LSTM
  # num_layers: number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRU/LSTMs together to form a stacked GRU/LSTM,
  #             with the second GRU/LSTM taking in outputs of the first GRU/LSTM and computing the final results.
  # fc1_out_dim: number of dimensions of the first fully connected network
  # pooling_mode: available options are 'attention', 'average', 'maximum', 'context'
  # dropout: if non-zero, introduces a Dropout layer on the outputs of each LSTM/GRU layer except the last layer, with dropout probability equal to dropout.
  # bias: XXX
  # rnn_hidden_dim: XXX
  # max_seq_len: XXX 
  # embedding_dim: XXX
  # output_dim: XXX
  # learning_rate: XXX
  # optimizer: XXX
  # epochs: XXX
  # batch_size: XXX 
  # dl_shuffle: shuffle when creating DataLoader
  # random_seed: XXX
  # training: train a new model?
  # validation: validate after each epoch using the validation set?
  # train_proportion: XXX
  # val_proportion: XXX
  # test_proportion: XXX
  # create_tensor_board: False or path to the directory

  main_architecture: "gru"
  mode: "char"
  bidirectional: True
  num_layers: 2
  fc1_out_dim: 120
  pooling_mode: 'context_layers_simple'
  gru_dropout: 0.01
  fc_dropout: [0.01, 0.01]  # fully-connected layers dropout depends on the number of fc layers (currently there are two)
  att_dropout: [0.01, 0.01] # attention layer dropout depends on the number of attention layers (currently there are two)
  bias: True

  rnn_hidden_dim: 60
  max_seq_len: 120
  embedding_dim: 60
  output_dim: 2

  learning_rate: 0.001
  optimizer: adam
  epochs: 5
  batch_size: 256
  dl_shuffle: True 
  random_seed: 123

  training: True
  validation: True
  train_proportion: 0.7
  val_proportion: 0.15
  test_proportion: 0.15

  #create_tensor_board: "./tb_gru_test" 
  create_tensor_board: False

  #Layers to freeze during fine-tuning
  layers_to_freeze: ["emb", "gru_1", "attn"]

inference:
 candidate_mode:
  output_vectors: "./embed_candidates/rnn"
  output_test_class: "./df/candidates.df" # XXX rename test_class
  overwrite: True

 query_mode:
  output_vectors: "./embed_queries/rnn"
  output_test_class: "./df/queries.df" # XXX rename test_class
  overwrite: True
